{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Dict, Iterator\n",
    "from functools import partial\n",
    "import pathlib\n",
    "import random\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Conv1D,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    "    MaxPooling1D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Softmax,\n",
    ")\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = pathlib.Path(\"data\")\n",
    "OUTPUT_PATH = pathlib.Path(\"output_ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(\n",
    "    data_path: str,\n",
    "    win_len: int,\n",
    "    win_stride: int = 1,\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[int, str]]:\n",
    "    \"\"\"Read CSV logs, discard columns, and segment data into windows.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Dataset path.\n",
    "        win_len (int): Window length (#samples).\n",
    "        win_stride (int): Window stride (#samples). Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, Dict[int, str]]:\n",
    "            Features, labels, labels_dict.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    labels_dict: Dict[int, str] = {}\n",
    "    for log_path in pathlib.Path(data_path).rglob(\"*.csv\"):\n",
    "        data_frame = pd.read_csv(log_path, sep=\",\", header=2, dtype=np.float32)\n",
    "        label, activity = log_path.parent.name.split(\"_\", maxsplit=1)\n",
    "        label = int(label)\n",
    "        labels_dict[label] = activity\n",
    "        win_list = []\n",
    "        idx = 0\n",
    "        while idx < len(data_frame) - win_len:\n",
    "            win_list.append(data_frame.values[idx : idx + win_len])\n",
    "            idx += win_stride\n",
    "        X += win_list\n",
    "        y += [label] * len(win_list)\n",
    "    return (np.array(X), to_categorical(y), labels_dict)\n",
    "\n",
    "\n",
    "def CNN_model(\n",
    "    inp_shape: Tuple[int, int],\n",
    "    out_shape: int,\n",
    "    hidden: int,\n",
    "    filters: int,\n",
    "    kernel_size: int,\n",
    "    pool_size: int,\n",
    "    dropout: Optional[float] = None,\n",
    "    learning_rate: float = 1e-3,\n",
    ") -> Model:\n",
    "    \"\"\"Create Convolutional Neural Network (CNN) using 1D layers.\n",
    "\n",
    "    Args:\n",
    "        inp_shape (Tuple[int, int]): Input shape (#window_length, #channels).\n",
    "        out_shape (int): Output shape (#classes).\n",
    "        hidden (int): Number of hidden layers.\n",
    "        filters (int): Number of convolutional filters in each hidden layer.\n",
    "        kernel_size (int): Size of kernel in each convolutional layer.\n",
    "        pool_size (int): Pool size of max pooling layer.\n",
    "        dropout (float | None): Dropout rate in final dense layer.\n",
    "            Defaults to None.\n",
    "        learning_rate (float): Adam optimizer learning rate. Defaults to 1e-3.\n",
    "\n",
    "    Returns:\n",
    "        Model: 1D convolutional Neural Network.\n",
    "    \"\"\"\n",
    "    x = Input(shape=inp_shape, name=\"input\")\n",
    "    model_input = x\n",
    "    for i in range(hidden):\n",
    "        x = Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=\"same\",\n",
    "            activation=None,\n",
    "            kernel_regularizer=\"l2\",\n",
    "            name=f\"conv_hidden_{i}\",\n",
    "        )(x)\n",
    "        x = BatchNormalization(scale=True)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = MaxPooling1D(pool_size=pool_size)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(\n",
    "        units=out_shape,\n",
    "        activation=None,\n",
    "        kernel_regularizer=\"l2\",\n",
    "        name=\"dense_out\",\n",
    "    )(x)\n",
    "    if dropout is not None:\n",
    "        x = Dropout(rate=dropout)(x)\n",
    "    model_output = Softmax(name=\"output\")(x)\n",
    "    model = Model(\n",
    "        model_input,\n",
    "        model_output,\n",
    "        name=\"cnn_\" + \"x\".join([str(filters)] * hidden),\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_model_history(model: Model, history: History) -> None:\n",
    "    \"\"\"Plot model training history.\n",
    "\n",
    "    Args:\n",
    "        model (Model): Keras model instance.\n",
    "        history (History): Training history instance.\n",
    "    \"\"\"\n",
    "    _, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "    for metric, ax in zip([\"accuracy\", \"loss\"], axes):\n",
    "        ax.set_title(f\"{model.name} {metric}\")\n",
    "        ax.plot(history.history[f\"{metric}\"])\n",
    "        ax.plot(history.history[f\"val_{metric}\"])\n",
    "        ax.set_ylabel(f\"{metric}\")\n",
    "        ax.set_xlabel(\"epoch\")\n",
    "        ax.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    y_test: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    labels_dict: Dict[int, str],\n",
    "    title: Optional[str] = None,\n",
    ") -> None:\n",
    "    \"\"\"Plot confusion matrix with given class names.\n",
    "\n",
    "    Args:\n",
    "        y_test (np.ndarray): True labels.\n",
    "        y_pred (np.ndarray): Predicted labels.\n",
    "        labels_dict (Dict[int, str]): Class value to semantic label mapping.\n",
    "        title (str | None): Plot figure title. Defaults to None.\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    if title is None:\n",
    "        title = \"Confusion Matrix\"\n",
    "    ax.set_title(title)\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "    labels = list(labels_dict.values())[:len(cm)]\n",
    "    df_cm = pd.DataFrame(cm, labels, labels)\n",
    "    sns.heatmap(df_cm, ax=ax, annot=True, annot_kws={\"size\": 12}, fmt=\".2f\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data from CSV logs and segment signals into 2s windows without overlap\n",
    "X, y, LABELS_DICT = load_dataset(DATA_PATH, win_len=52, win_stride=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training set and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3)\n",
    "\n",
    "# Split dataset into training set and validation set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, stratify=y_train, test_size=0.1)\n",
    "\n",
    "print(f\"Training set:   {len(y_train):>4} samples\")\n",
    "print(f\"Validation set: {len(y_valid):>4} samples\")\n",
    "print(f\"Testing set:    {len(y_test):>4} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of 1D-CNN model with 3 hidden layers\n",
    "model = CNN_model(\n",
    "    inp_shape=X.shape[1:],\n",
    "    out_shape=y.shape[1],\n",
    "    hidden=3,\n",
    "    filters=8,\n",
    "    kernel_size=3,\n",
    "    pool_size=2,\n",
    "    dropout=0.1,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = OUTPUT_PATH / f\"{model.name}.h5\"\n",
    "\n",
    "# Start model training using validation set to track progress\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            verbose=1,\n",
    "            patience=20,\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(keras_model),\n",
    "            patience=20,\n",
    "            monitor=\"val_loss\",\n",
    "            save_freq=\"epoch\",\n",
    "            save_best_only=True,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Load best model weights\n",
    "model.load_weights(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model_history(model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nPrediction accuracy: {acc:.02%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain predictions from model\n",
    "y_pred = model.predict(X_test)\n",
    "plot_confusion_matrix(\n",
    "    np.argmax(y_test, axis=1),\n",
    "    np.argmax(y_pred, axis=1),\n",
    "    LABELS_DICT,\n",
    "    f\"Confusion matrix: {model.name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_representative_data(X: np.ndarray) -> Iterator[tf.lite.RepresentativeDataset]:\n",
    "    \"\"\"Create iterator for TFLite API to estimate dynamic range from data.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Feature dataset.\n",
    "\n",
    "    Yields:\n",
    "        Iterator[tf.lite.RepresentativeDataset]: Dataset iterator.\n",
    "    \"\"\"\n",
    "    for val in tf.data.Dataset.from_tensor_slices(X).batch(1).take(100):\n",
    "        yield [val]\n",
    "\n",
    "\n",
    "def run_tflite_model(interpreter: tf.lite.Interpreter, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Helper function to run TFLite model inference and get predictions.\n",
    "\n",
    "    Args:\n",
    "        interpreter (tf.lite.Interpreter): TFLite interpreter.\n",
    "        X (np.ndarray): Feature dataset.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Model predictions.\n",
    "    \"\"\"\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    predictions = np.zeros((len(X),), dtype=int)\n",
    "    for i, x in enumerate(X):\n",
    "        x = np.expand_dims(x, axis=0).astype(input_details[\"dtype\"])\n",
    "        interpreter.set_tensor(input_details[\"index\"], x)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "        predictions[i] = output.argmax()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TFLite converter\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = partial(get_representative_data, X_train)\n",
    "\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "# Set the input and output tensors to float32\n",
    "converter.inference_input_type = tf.float32\n",
    "converter.inference_output_type = tf.float32\n",
    "tflite_model_quant = converter.convert()\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain predictions from the quantized model\n",
    "y_pred = run_tflite_model(interpreter, X_test)\n",
    "acc = np.sum(y_pred == np.argmax(y_test, axis=1)) / len(y_pred)\n",
    "print(f\"Prediction accuracy: {acc:.02%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    np.argmax(y_test, axis=1),\n",
    "    y_pred,\n",
    "    LABELS_DICT,\n",
    "    f\"Confusion matrix: q{model.name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model\n",
    "tflite_model = OUTPUT_PATH / f\"q{model.name}.tflite\"\n",
    "tflite_model.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save validation data\n",
    "testset = OUTPUT_PATH / \"har_testset.npz\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "np.savez(testset, m_inputs=X_test, m_outputs=y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
