{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Dict, Iterator\n",
    "from functools import partial\n",
    "import pathlib\n",
    "import shutil\n",
    "import random\n",
    "import logging\n",
    "import copy\n",
    "logging.disable(logging.WARNING)\n",
    "import os\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import onnx\n",
    "from onnxruntime import InferenceSession\n",
    "from onnxruntime.quantization import quantize_static, QuantFormat, QuantType, CalibrationDataReader\n",
    "from onnxruntime.quantization.shape_inference import quant_pre_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Set constants of the experiment</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = pathlib.Path(\"data\")\n",
    "OUTPUT_PATH = pathlib.Path(\"output_ipynb\")\n",
    "DENSE_UNITS = 64\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 32\n",
    "PATIENCE = 100\n",
    "MODEL_NAME = f'dense_{DENSE_UNITS}x{DENSE_UNITS // 2}.onnx'\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Define features extraction and data preprocessing utilities</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zcr(win: np.ndarray):\n",
    "    \"\"\"Zero-crossing rate.\n",
    "\n",
    "    Args:\n",
    "        win (np.ndarray): array of samples.\n",
    "\n",
    "    Returns:\n",
    "        ZCR value.\n",
    "    \"\"\"\n",
    "    zcr = 0\n",
    "    curr_sign = np.sign(win[0])\n",
    "    for sample in win:\n",
    "        if np.sign(sample) * curr_sign < 0:\n",
    "            curr_sign = np.sign(sample)\n",
    "            zcr += 1\n",
    "    return zcr\n",
    "\n",
    "def mcr(win: np.ndarray, axis: int = 0):\n",
    "    \"\"\"Mean-crossing rate along the specified axis.\n",
    "\n",
    "    Args:\n",
    "        win (np.ndarray): array of samples.\n",
    "        axis (int): axis index.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: MCR values along x, y, and z axes.\n",
    "    \"\"\"\n",
    "    means = np.mean(win, axis=axis)\n",
    "    feat_list = []\n",
    "    if axis == 0:\n",
    "        win = win.transpose()\n",
    "    for ax, m in enumerate(means):\n",
    "        mcr = 0\n",
    "        detrended_win = win[ax] - m\n",
    "        feat_list.append(zcr(detrended_win))\n",
    "\n",
    "    return np.array(feat_list)\n",
    "         \n",
    "        \n",
    "def compute_features(win: np.ndarray):\n",
    "    \"\"\"Compute the set of features of interest on the given window of signal.\n",
    "        - mean: contains information about the device orientation.\n",
    "        - variance: contains information about the intensity of the activity.\n",
    "        - mean-crossing rate: contains information about the fundamental frequency of the activity.\n",
    "\n",
    "    Args:\n",
    "        win (np.ndarray): array of samples.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, List[str]]: (3 x axes)-dimensional data-point (3 features x number of input axes), features name.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_point = np.array([])\n",
    "    features = [np.mean, np.var, mcr]\n",
    "    for feat in features:\n",
    "        data_point = np.concatenate([data_point, feat(win, axis=0)])\n",
    "\n",
    "    return data_point, ['mean', 'var', 'mcr']\n",
    "\n",
    "def load_dataset(\n",
    "    data_path: str,\n",
    "    win_len: int,\n",
    "    win_stride: int = 1,\n",
    ") -> Tuple[np.ndarray, np.ndarray, Dict[int, str]]:\n",
    "    \"\"\"Read CSV logs, discard columns, and segment data into windows.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Dataset path.\n",
    "        win_len (int): Window length (#samples).\n",
    "        win_stride (int): Window stride (#samples). Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, Dict[int, str], Dict[int, str]]:\n",
    "            Features, labels, labels_dict, features_dict (index, name).\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    labels_dict: Dict[int, str] = {}\n",
    "    for log_path in pathlib.Path(data_path).rglob(\"*.csv\"):\n",
    "        data_frame = pd.read_csv(log_path, sep=\",\", header=2, dtype=np.float32)\n",
    "        label, activity = log_path.parent.name.split(\"_\", maxsplit=1)\n",
    "        label = int(label)\n",
    "        labels_dict[label] = activity\n",
    "        win_list = []\n",
    "        idx = 0\n",
    "        while idx < len(data_frame) - win_len:\n",
    "            data_point, features_name = compute_features(data_frame.values[idx : idx + win_len])\n",
    "            win_list.append(data_point)\n",
    "            idx += win_stride\n",
    "        X += win_list\n",
    "        y += [label] * len(win_list)\n",
    "\n",
    "    features_dict = {idx : '_'.join(t) for idx, t in enumerate(product(features_name, ['x', 'y', 'z']))}\n",
    "    return (np.array(X), np.array(y), labels_dict, features_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Define training utilities</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class specialization to be used by a Pytorch dataloader.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        input data points.\n",
    "    y : np.ndarray\n",
    "        labels corresponding to input data points\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = copy.deepcopy(X)\n",
    "        self.y = copy.deepcopy(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Return data point, label couple in position idx.\n",
    "\n",
    "        Args:\n",
    "            idx (int): current index.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, int]: data point, label couple in position idx.\n",
    "        \"\"\"\n",
    "        return torch.from_numpy(self.X[idx]).float(), self.y[idx]\n",
    "\n",
    "def evaluate_predictions(output: torch.Tensor, target: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Compute matching between predictions array and the target array.\n",
    "\n",
    "    Args:\n",
    "        output (torch.Tensor): output predictions.\n",
    "        target (torch.Tensor): ground truth.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: outputs, counts of matching predictions per output.\n",
    "    \"\"\"\n",
    "    pred = torch.reshape(torch.topk(output, 1)[1], (-1,))\n",
    "    return torch.unique(pred[pred == target], return_counts=True)\n",
    "    \n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    training_loader: torch.utils.data.DataLoader,\n",
    "    batch_size: int = 250\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Epoch training step.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): input model to be trained.\n",
    "        optimizer (optim.Optimizer): loss function optimizer.\n",
    "        loss_fn (nn.Module): loss function.\n",
    "        training_loader (torch.utils.data.DataLoader): training set data loader.\n",
    "        batch_size (int): size of every training.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[int, int]: average training loss, average training accuracy.\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    running_accuracy = 0.\n",
    "    last_accuracy = 0.\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_accuracy += evaluate_predictions(outputs, labels)[1].sum().item() / batch_size\n",
    "        \n",
    "        if i % len(training_loader) == len(training_loader) - 1:\n",
    "            last_loss = running_loss / len(training_loader) # loss per batch\n",
    "            last_accuracy = running_accuracy / len(training_loader)\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "\n",
    "    return last_loss, last_accuracy\n",
    "    \n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    epochs: int,\n",
    "    optimizer: optim.Optimizer,\n",
    "    loss_fn: nn.Module,\n",
    "    training_loader: torch.utils.data.DataLoader,\n",
    "    batch_size: int = 250,\n",
    "    patience: int = 20\n",
    ") -> Dict[str, list[float]]:\n",
    "    \"\"\"Epochs training scheduler.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): input model to be trained.\n",
    "        optimizer (optim.Optimizer): loss function optimizer.\n",
    "        lr_scheduler (optim.lr_scheduler.LRScheduler): learning rate scheduler.\n",
    "        loss_fn (nn.Module): loss function.\n",
    "        training_loader (torch.utils.data.DataLoader): training set data loader.\n",
    "        batch_size (int): size of every training batch.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, list[float]]: average training / validation loss history, average training / validation accuracy history.\n",
    "    \"\"\"\n",
    "    if os.path.exists(OUTPUT_PATH / 'models_pytorch'):\n",
    "        shutil.rmtree(OUTPUT_PATH / 'models_pytorch')\n",
    "    os.mkdir(OUTPUT_PATH / 'models_pytorch')\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    early_stop = 0\n",
    "    best_vloss = 1000000.\n",
    "    history_vloss = []\n",
    "    history_vaccuracy = []\n",
    "    history_loss = []\n",
    "    history_accuracy = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        avg_loss, avg_accuracy = train_one_epoch(model, optimizer, criterion, train_dataloader, batch_size)\n",
    "        \n",
    "        history_loss.append(avg_loss)\n",
    "        history_accuracy.append(avg_accuracy)\n",
    "        running_vloss = 0.0\n",
    "        running_vaccuracy = 0.0\n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        model.eval()\n",
    "\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(valid_dataloader):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = criterion(voutputs, vlabels)\n",
    "                running_vloss += vloss\n",
    "                running_vaccuracy += evaluate_predictions(voutputs, vlabels)[1].sum().item() / len(vlabels)\n",
    "\n",
    "            avg_vloss = running_vloss / len(valid_dataloader)\n",
    "            avg_vaccuracy = running_vaccuracy / len(valid_dataloader)\n",
    "            history_vloss.append(avg_vloss)\n",
    "            history_vaccuracy.append(avg_vaccuracy)\n",
    "\n",
    "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "        print('ACCURACY train {} valid {}'.format(avg_accuracy, avg_vaccuracy))\n",
    "\n",
    "        early_stop += 1\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = OUTPUT_PATH / 'models_pytorch' / f'dense_{timestamp}_{epoch}'\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            early_stop = 0\n",
    "\n",
    "        if early_stop == patience:\n",
    "            break\n",
    "\n",
    "    # Note: add Softmax layer to have class probabilities in output during inference\n",
    "    model.add_module(f'{len(model)}', nn.Softmax(dim=1))\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    torch.onnx.export(\n",
    "        model, torch.from_numpy(np.array([X_test[0]])).float(), f=OUTPUT_PATH / MODEL_NAME,\n",
    "        export_params = True\n",
    "    )\n",
    "    shutil.rmtree(OUTPUT_PATH / 'models_pytorch')\n",
    "\n",
    "    return {'loss': history_loss, 'val_loss': history_vloss, 'accuracy': history_accuracy, 'val_accuracy': history_vaccuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Define data visualization utilities</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_history(model_name: str, history: Dict[str, np.ndarray]) -> None:\n",
    "    \"\"\"Plot model training history.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Keras model instance.\n",
    "        history (Dict[str, np.ndarray]): Training history instance.\n",
    "    \"\"\"\n",
    "    _, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True)\n",
    "    for metric, ax in zip([\"accuracy\", \"loss\"], axes):\n",
    "        ax.set_title(f\"{model_name} {metric}\")\n",
    "        ax.plot(history[f\"{metric}\"])\n",
    "        ax.plot(history[f\"val_{metric}\"])\n",
    "        ax.set_ylabel(f\"{metric}\")\n",
    "        ax.set_xlabel(\"epoch\")\n",
    "        ax.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    y_test: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    labels_dict: Dict[int, str],\n",
    "    title: Optional[str] = None,\n",
    ") -> None:\n",
    "    \"\"\"Plot confusion matrix with given class names.\n",
    "\n",
    "    Args:\n",
    "        y_test (np.ndarray): True labels.\n",
    "        y_pred (np.ndarray): Predicted labels.\n",
    "        labels_dict (Dict[int, str]): Class value to semantic label mapping.\n",
    "        title (str | None): Plot figure title. Defaults to None.\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    if title is None:\n",
    "        title = \"Confusion Matrix\"\n",
    "    ax.set_title(title)\n",
    "    cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
    "    labels = list(labels_dict.values())[:len(cm)]\n",
    "    df_cm = pd.DataFrame(cm, labels, labels)\n",
    "    sns.heatmap(df_cm, ax=ax, annot=True, annot_kws={\"size\": 12}, fmt=\".2f\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_features_distribution(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    features_dict: Dict[int, str],\n",
    "    labels_dict: Dict[int, str],\n",
    "    n_rows: int,\n",
    "    n_cols: int\n",
    ") -> None:\n",
    "    \"\"\"Plot the features distribution over target classes.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): data points.\n",
    "        y (np.ndarray): labels corresponding to data points.\n",
    "        features_dict (Dict[int, str]): mapping between features index and features semantic.\n",
    "        labels_dict (Dict[int, str]): mapping between labels index and labels semantic.\n",
    "        n_rows (int): number of rows of the subplots.\n",
    "        n_cols (int): number of columns of the subplots.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(n_rows, n_cols, sharex=True, figsize=(16, 16))\n",
    "    fig.suptitle('Normalized features distribution over labels')\n",
    "    xlabs = [(key, val) for key, val in labels_dict.items()]\n",
    "    xlabs.sort(key=lambda el: el[0])\n",
    "    xlabs = [el[1] for el in xlabs]\n",
    "    row_idx, col_idx = 0, 0\n",
    "\n",
    "    for feat_idx, feat_name in features_dict.items():\n",
    "        dist = [[] for i in labels_dict.keys()]\n",
    "        for data_point, lab in zip(X, y):\n",
    "            dist[lab].append(data_point[feat_idx])\n",
    "        ax[row_idx][col_idx].set_title(feat_name)\n",
    "        ax[row_idx][col_idx].set_xlabel('Label')\n",
    "        ax[row_idx][col_idx].boxplot(dist, tick_labels=xlabs)\n",
    "        if col_idx == n_cols - 1:\n",
    "            row_idx += 1\n",
    "        col_idx = (col_idx + 1) % n_cols\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Extract, preprocess, and inspect the dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data from CSV logs and segment signals into 2s windows without overlap\n",
    "X, y, labels_dict, features_dict = load_dataset(DATA_PATH, win_len=52, win_stride=52)\n",
    "\n",
    "# Split dataset into training set and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3)\n",
    "\n",
    "# Split dataset into training set and validation set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, stratify=y_train, test_size=0.1)\n",
    "\n",
    "print(f\"Training set:   {len(y_train):>4} samples\")\n",
    "print(f\"Validation set: {len(y_valid):>4} samples\")\n",
    "print(f\"Testing set:    {len(y_test):>4} samples\")\n",
    "\n",
    "# Note: features normalization guarantees high accuracy results both before and after post-training quantization and a smoother training curve\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "train_dataloader = DataLoader(FeaturesDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(FeaturesDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n",
    "valid_dataloader = DataLoader(FeaturesDataset(X_valid, y_valid), batch_size=BATCH_SIZE, shuffle=False)\n",
    "plot_features_distribution(np.concatenate([X_train, X_valid, X_test]), np.concatenate([y_train, y_valid, y_test]), features_dict, labels_dict, 3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Define the model and launch the training</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple Dense model to classify input data points\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X.shape[1], DENSE_UNITS),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(DENSE_UNITS, DENSE_UNITS // 2),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(DENSE_UNITS // 2, len(set(y))),\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "history = train(\n",
    "    model,\n",
    "    EPOCHS,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_dataloader,\n",
    "    BATCH_SIZE,\n",
    "    PATIENCE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Plot training statistics and inspect input features distribution</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history('har_dense', history)\n",
    "\n",
    "model.eval()\n",
    "running_taccuracy = 0.\n",
    "running_tloss = 0.\n",
    "pred = []\n",
    "with torch.no_grad():\n",
    "    for i, tdata in enumerate(test_dataloader):\n",
    "        tinputs, tlabels = tdata\n",
    "        toutputs = model(tinputs)\n",
    "        tloss = criterion(toutputs, tlabels)\n",
    "        running_tloss += tloss\n",
    "        running_taccuracy += evaluate_predictions(toutputs, tlabels)[1].sum().item() / len(tlabels)\n",
    "        pred += list(torch.reshape(torch.topk(toutputs, 1)[1], (-1,)))\n",
    "avg_taccuracy = running_taccuracy / len(test_dataloader)\n",
    "print(f'\\nPrediction accuracy: {avg_taccuracy:.02%}')\n",
    "plot_confusion_matrix(\n",
    "    y_test,\n",
    "    pred,\n",
    "    labels_dict,\n",
    "    f'Confusion matrix: har_dense'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Post-training QDQ per-tensor quantization to 8-bit precision</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataReader(CalibrationDataReader):\n",
    "    \"\"\"\n",
    "    Custom CalibrationDataReader to feed the post-quantization algorithm with calibration data samples.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    calibration_data : np.ndarray\n",
    "        calibration data collection.\n",
    "    index : int\n",
    "        cursor to iterate over data points in the calibration data collection.\n",
    "    \"\"\"\n",
    "    def __init__(self, calibration_data):\n",
    "        self.calibration_data = calibration_data\n",
    "        self.index = 0\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.index < len(self.calibration_data):\n",
    "            data = np.array([self.calibration_data[self.index]]).astype(np.float32)\n",
    "            self.index += 1\n",
    "            return { f'onnx::Gemm_0': data }\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# Statically quantize the model\n",
    "data_reader = CustomDataReader(X_train)\n",
    "quant_pre_process(\n",
    "    OUTPUT_PATH / MODEL_NAME,\n",
    "    OUTPUT_PATH / f'q{MODEL_NAME}',\n",
    "    skip_symbolic_shape=False\n",
    ")\n",
    "\n",
    "quantize_static(\n",
    "    OUTPUT_PATH / MODEL_NAME,\n",
    "    OUTPUT_PATH / f'q{MODEL_NAME}',\n",
    "    data_reader,\n",
    "    quant_format=QuantFormat.QDQ, # Note: this is the recommended format to ensure optimal conversion using stedgeai\n",
    "    activation_type=QuantType.QInt8,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    per_channel=False, # Note: since all the activations are 1-D vectors, use per-tensor quantization to ensure optimal conversion using stedgeai\n",
    "    nodes_to_exclude=['/7/Softmax'] # Note: avoid quantization of Softmax layer outputs to have floating point probabilities during inference\n",
    ")\n",
    "\n",
    "# Run predictions to evaluate accuracy after quantization\n",
    "ort_int8_sess = InferenceSession(OUTPUT_PATH / f'q{MODEL_NAME}', providers=['CPUExecutionProvider'])\n",
    "acc = 0.\n",
    "for sample, label in zip(X_test, y_test):\n",
    "    ort_inputs = {ort_int8_sess.get_inputs()[0].name: np.array([sample]).astype(np.float32)}\n",
    "    ort_int8_outs = ort_int8_sess.run(None, ort_inputs)[0]\n",
    "    if np.argmax(ort_int8_outs) == label:\n",
    "        acc += 1\n",
    "\n",
    "print(f'\\nQuantized model accuracy: {acc / len(y_test):.02%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Save the test set in npz format and print out standard scaler estimated parameters (to be copied-pasted to use them in inference)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = OUTPUT_PATH / \"har_testset.npz\"\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "np.savez(testset, m_inputs=X_test, m_outputs=np.eye(len(set(y_test)), dtype='float')[y_test])\n",
    "print('static const float features_mean[STAI_NETWORK_IN_1_CHANNEL] = {\\n\\t' + ',\\n\\t'.join([str(x) + 'f' for x in scaler.mean_]) + '\\n};\\n')\n",
    "print('static const float features_inv_std_dev[STAI_NETWORK_IN_1_CHANNEL] = {\\n\\t' + ',\\n\\t'.join([str(1 / x) + 'f' for x in scaler.scale_]) + '\\n};')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
